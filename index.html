<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Divide, Distill, and Conquer: A Specialist–Generalist Framework for CIFAR‑10</title>

  <!-- Social‑metadata -->
  <meta property="og:title" content="Divide, Distill, and Conquer: A Specialist–Generalist Framework for CIFAR‑10" />
  <meta name="twitter:title" content="Divide, Distill, and Conquer: A Specialist–Generalist Framework for CIFAR‑10" />
  <meta name="description" content="A specialist–generalist approach on CIFAR‑10 using model distillation." />
  <meta property="og:description" content="A specialist–generalist approach on CIFAR‑10 using model distillation." />
  <meta name="twitter:description" content="A specialist–generalist approach on CIFAR‑10 using model distillation." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />

  <!-- Google Fonts & Bootstrap -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap" rel="stylesheet">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
        integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N"
        crossorigin="anonymous">

  <!-- Custom styles -->
  <link href="style.css" rel="stylesheet">

  <!-- MathJax for equations -->
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <!-- NAVBAR -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top">
    <div class="container">
      <a class="navbar-brand" href="#">Model Distillation Blog</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse"
              data-target="#navbarNav" aria-controls="navbarNav"
              aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="#introduction">Introduction</a></li>
          <li class="nav-item"><a class="nav-link" href="#related-work">Related Work</a></li>
          <li class="nav-item"><a class="nav-link" href="#distillation">Distillation</a></li>
          <li class="nav-item"><a class="nav-link" href="#ensemble">Ensemble Strategy</a></li>
          <li class="nav-item"><a class="nav-link" href="#experiments">Experiments</a></li>
          <li class="nav-item"><a class="nav-link" href="#math">Math Deep‑Dive</a></li>
          <li class="nav-item"><a class="nav-link" href="#discussion">Discussion</a></li>
          <li class="nav-item"><a class="nav-link" href="#team">Team & Code</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- HERO -->
  <header class="hero text-light text-center d-flex align-items-center">
    <div class="container">
      <h1 class="display-4 font-weight-bold">Divide, Distill, and Conquer</h1>
      <p class="lead mt-3">A Specialist–Generalist Framework for CIFAR‑10 using Model Distillation</p>
    </div>
  </header>

  <!-- MAIN CONTENT -->
  <main class="mt-5 pt-5">
    <div class="container">

      <!-- Introduction -->
      <!-- Introduction -->
<section id="introduction" class="my-5">
  <h2 class="section-title">Introduction</h2>
  <p>Recent advancements in neural networks have dramatically increased image-classification performance, yet deploying these large models remains challenging due to latency and memory constraints. While Geoffrey Hinton's model distillation technique successfully compresses complex ensembles into smaller networks, a critical question emerges: <strong>Can strategic collaboration between specialized expert models and generalist networks unlock new accuracy frontiers on benchmarks like CIFAR-10, without sacrificing deployability?</strong></p>
  
  <p>This article investigates how a <strong>specialist-generalist framework</strong>—combining domain-specific experts with broad-coverage models—overcomes key limitations of standalone distillation approaches. We dissect whether fine-grained class knowledge from specialists (e.g., distinguishing deer from horses) can synergize with a generalist's holistic understanding to achieve state-of-the-art results, while remaining efficient enough for real-world applications.</p>
</section>

      <!-- Related Work -->
      <section id="related-work" class="my-5">
        <h2 class="section-title">Related Work</h2>
        <p>This project builds directly upon the seminal work by Hinton, Vinyals, and Dean (2015) in their paper <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener noreferrer">"Distilling the Knowledge in a Neural Network."</a> In their breakthrough research, they introduced the concept of knowledge distillation as a model compression technique, demonstrating that the "dark knowledge" captured in soft probability distributions from teacher models contains valuable information beyond hard class labels.</p>
        <p>While Hinton et al. primarily focused on distilling knowledge from large ensembles into a single network using a uniform softmax temperature, our work extends their approach by implementing a specialized ensemble structure with domain-specific expert models. We particularly focus on their specialist-generalist ensemble concept, where they introduced "a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes." Our implementation adapts this approach for the CIFAR-10 dataset with semantic grouping of specialists (animals vs. vehicles), demonstrating significant accuracy improvements while maintaining deployment efficiency through distillation.</p>
      </section>

      <!-- Distillation -->
      <section id="distillation" class="my-5">
        <h2 class="section-title">What is Model Distillation?</h2>
        <p>Model distillation transfers knowledge from a <em>cumbersome</em> model—often an ensemble—into a lightweight student. Rather than learning solely from hard, one‑hot labels, the student learns from the teacher's <strong>soft targets</strong>: the entire probability vector output by a higher‑temperature soft‑max. These probabilities embed rich information about inter‑class similarity.</p>
        <p>For example, the teacher might classify an image as "cat" with 60% confidence, "dog" with 35%, and only 0.1% for "truck." Those relative values show the student that cats and dogs share visual traits, which is precisely the nuance that improves generalisation.</p>
          <figure class="my-4 text-center">
            <img src="Picture2.png"
                  alt="Overview of the model distillation process"
                  class="img-fluid w-50">
            <figcaption>Figure 1 – Overview of the distillation process. <a href="https://www.youtube.com/watch?v=tG4MdpccG3Q" target="_blank">Image source</a></figcaption>
          </figure>
      </section>

      <!-- Ensemble Strategy -->
      <section id="ensemble" class="my-5">
        <h2 class="section-title">Generalist and Specialist Models</h2>
        <div class="row align-items-center">
          <div class="col-md-6">
            <p>Ensembles squeeze out extra accuracy. Ours contains:</p>
            <ul>
              <li><strong>Generalist Model</strong> — ResNet‑34 trained on all 10 CIFAR‑10 classes.</li>
              <li><strong>Specialist Models</strong> — Two ResNet‑18 networks, each fine‑tuned on a subset:</li>
              <ul>
                <li><em>Animals:</em> bird, cat, deer, dog, frog, horse</li>
                <li><em>Vehicles:</em> plane, car, ship, truck</li>
              </ul>
            </ul>
            
            <h3>How Specialist-Generalist Ensembles Work</h3>
            <p>Unlike traditional mixtures of experts where all models are consulted for every input, our approach uses a more efficient two-stage process:</p>
            
            <ol>
              <li><strong>Training Phase:</strong></li>
              <ul>
                <li>The generalist model is trained on the complete dataset first</li>
                <li>We analyze the confusion matrix to identify commonly confused class groups</li>
                <li>Specialist models are then trained specifically on these challenging subsets</li>
                <li>Specialists develop expertise in distinguishing between visually similar classes (e.g., different animal species)</li>
              </ul>
              
              <li><strong>Inference Phase:</strong></li>
              <ul>
                <li>Run the generalist to obtain top‑\(n\) candidate classes.</li>
                <li>Activate specialists whose label set overlaps those candidates.</li>
                <li>Fuse generalist and specialist distributions (KL‑divergence optimisation) to get the final prediction.</li>
              </ul>
            </ol>
            
            <p>This approach is computationally efficient because specialists are only activated when needed. For example, if the generalist confidently predicts "car" with high probability, only the vehicle specialist might be consulted, leaving the animal specialist dormant. This selective activation significantly reduces the computational overhead compared to always using all models.</p>
          </div>
          <div class="col-md-6 text-center">
            <img src="Picture3.png"
                  alt="Comparison of generalist and specialist models"
                  class="img-fluid">
            <figcaption>Figure 2 – CIFAR‑10 Dataset.</figcaption>
          </div>
        </div>
      </section>

      <!-- Experiments -->
      <section id="experiments" class="my-5">
        <h2 class="section-title">Experimental Results and Analysis</h2>
        <p>We trained the three networks on CIFAR‑10 and distilled their ensemble knowledge into a single student (ResNet‑18). Key findings:</p>

        <h3>Per‑Class Accuracy Improvement</h3>
        <table class="table table-striped">
          <thead>
            <tr><th>Class</th><th>Generalist (%)</th><th>Ensemble w/ Specialists (%)</th><th>Δ (%)</th></tr>
          </thead>
          <tbody>
            <tr><td>plane</td><td>83.30</td><td>82.00</td><td>−1.30</td></tr>
            <tr><td>car</td><td>91.30</td><td>90.50</td><td>−0.80</td></tr>
            <tr><td>bird</td><td>72.60</td><td>76.10</td><td>+3.50</td></tr>
            <tr><td>cat</td><td>64.80</td><td>67.40</td><td>+2.60</td></tr>
            <tr><td>deer</td><td>79.50</td><td>83.40</td><td>+3.90</td></tr>
            <tr><td>dog</td><td>72.70</td><td>77.20</td><td>+4.50</td></tr>
            <tr><td>frog</td><td>85.50</td><td>89.50</td><td>+4.00</td></tr>
            <tr><td>horse</td><td>85.90</td><td>88.00</td><td>+2.10</td></tr>
            <tr><td>ship</td><td>89.50</td><td>91.80</td><td>+2.30</td></tr>
            <tr><td>truck</td><td>87.90</td><td>89.00</td><td>+1.10</td></tr>
          </tbody>
        </table>

        <h3>Overall Performance</h3>
        <table class="table table-striped">
          <thead><tr><th>Model</th><th>Accuracy (%)</th></tr></thead>
          <tbody>
            <tr><td>Generalist (ResNet‑34)</td><td>81.30</td></tr>
            <tr><td>Specialist 1 (Animals, ResNet‑18)</td><td>82.37</td></tr>
            <tr><td>Specialist 2 (Vehicles, ResNet‑18)</td><td>91.35</td></tr>
            <tr><td><strong>Distilled Ensemble</strong></td><td><strong>83.49</strong></td></tr>
          </tbody>
        </table>
      </section>

      <!-- Math Deep‑Dive -->
      <section id="math" class="my-5">
        <h2 class="section-title">Math Deep‑Dive: How Knowledge Distillation Works</h2>

        <h3>1  Soft‑max with Temperature</h3>
        <p>The teacher converts logits \(z\) into probabilities by</p>
        <p class="equation">$$
          q_i(T)=\frac{\exp\!\bigl(z_i/T\bigr)}{\displaystyle\sum_j \exp\!\bigl(z_j/T\bigr)},\quad T\ge1.
        $$</p>

        <h3>2  Student Loss Function</h3>
        <p>We blend soft and hard targets:</p>
        <p class="equation">$$
          \mathcal{L}= \lambda\,T^{2}\,\operatorname{CE}\bigl(q(T),p(T)\bigr)
          +\bigl(1-\lambda\bigr)\,\operatorname{CE}\bigl(y,p(1)\bigr).
        $$</p>
        <p>Here \(p\) is the student distribution, \(y\) the one‑hot label, and \(\lambda\in[0,1]\).</p>

        <h3>3  Gradient ≈ Logit Matching</h3>
        <p>With \(\lambda=1\):</p>
        <p class="equation">$$
          \frac{\partial \mathcal{L}}{\partial z_i}
          =\frac{1}{T}\,\bigl(p_i(T)-q_i(T)\bigr).
        $$</p>
        <p>At high \(T\) this becomes an ℓ² regression on logits, making "matching logits" a special case.</p>

        <h3>4  Fusing Specialist &amp; Generalist Outputs</h3>
        <p>With generalist \(p^{\mathrm g}\) and active specialists \(p^{(m)}\):</p>
        <p class="equation">$$
          q^*=\arg\min_q\Bigl[\operatorname{KL}\!\bigl(p^{\mathrm g},q\bigr)+\!\!\sum_{m\in\mathcal{A}_k}\!\operatorname{KL}\!\bigl(p^{(m)},q\bigr)\Bigr].
        $$</p>

        <h3>5  Practical Tips</h3>
        <ul>
          <li>Search \(T\in[4,10]\); increase if student capacity is much smaller.</li>
          <li>Set \(\lambda\approx0.8\) so soft targets dominate.</li>
          <li>Use a higher learning rate—soft targets smooth the loss surface.</li>
        </ul>
      </section>

      <!-- Discussion / Conclusion -->
      <section id="discussion" class="my-5">
        <h2 class="section-title">Discussion</h2>
        
        <p>This study investigated the effectiveness of model distillation in transferring knowledge from a specialist–generalist ensemble to a compact student network for image classification on the CIFAR-10 dataset. The primary objective was to determine whether the accuracy benefits of ensemble methods, particularly those leveraging specialist models, could be retained in a single deployable model through distillation, thereby addressing the practical constraints of model size and inference latency.</p>
        
        <p>The results indicate that distilling the ensemble into a student model (ResNet-18) yields a notable improvement in overall accuracy compared to a generalist model alone. Specifically, the distilled student achieved an accuracy of 83.49%, surpassing the generalist ResNet-34 baseline (81.30%) and closely matching the ensemble's performance. Per-class analysis revealed that the most pronounced gains were observed in animal classes, with improvements of up to 4.5 percentage points for classes such as "dog" and "deer." These findings suggest that specialist models, when focused on semantically similar or historically confused classes, are particularly effective at capturing fine-grained distinctions that generalist models may overlook.</p>
        
        <p>The observed performance gains can be attributed to the use of soft targets during distillation. Soft targets encode the teacher model's uncertainty and the relationships between classes, providing the student with richer supervisory signals than hard labels alone. This aligns with previous literature, which has shown that distillation acts as a regularizer and enables smaller models to generalize better by leveraging the "dark knowledge" present in the teacher's output probabilities.</p>
        
        <p>Despite these improvements, the study also identified certain limitations. Not all classes benefited equally from the specialist–generalist approach; some vehicle classes experienced marginal declines in accuracy. This suggests that the current grouping of specialists may not fully capture the underlying structure of class confusion within the dataset. Future research could explore automated or data-driven methods for specialist assignment, potentially leveraging clustering techniques based on confusion matrices or learned feature representations.</p>
        
        <p>Another area for further investigation involves the selection and scheduling of distillation hyperparameters, such as the softmax temperature and the blending coefficient \(\lambda\). The current approach relies on static values, but adaptive or dynamic strategies may yield additional performance gains. Moreover, while the study demonstrated the effectiveness of the approach on CIFAR-10, extending this methodology to larger and more complex datasets, such as ImageNet, remains an open challenge.</p>
        
        <p>In summary, this work demonstrates that model distillation can effectively compress the knowledge of a specialist–generalist ensemble into a single, efficient model, achieving high accuracy while meeting the practical requirements of real-world deployment. The results highlight the potential of combining ensemble learning and distillation for scalable and efficient deep learning systems, while also identifying promising directions for future research, including more sophisticated specialist structures and adaptive distillation techniques.</p>
      </section>
      

      <!-- References -->
      <section id="references" class="my-5">
        <h2 class="section-title">References</h2>
        <ul class="list-unstyled mb-0">
          <li>[1] <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener noreferrer">G. Hinton <em>et al.</em>, "Distilling the Knowledge in a Neural Network," arXiv 1503.02531 (2015)</a></li>
        </ul>
      </section>

      <!-- Team -->
      <section id="team" class="my-5">
        <h2 class="section-title">Team & Code</h2>
        <p><strong>Authors:</strong> Rohan Bhatane, Ayaazuddin Mohammad</p>
        <p><strong>GitHub:</strong> <a href="https://github.com/ayaazuddin/Model-Distillation" target="_blank">github.com/ayaazuddin/Model-Distillation</a></p>
      </section>

    </div><!-- /.container -->
  </main>

  <!-- FOOTER -->
  <footer class="footer bg-primary text-light py-4">
    <div class="container text-center">
      <a href="https://cs7150.baulab.info/" class="text-light">About CS 7150</a>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
          integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
          crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
          crossorigin="anonymous"></script>
</body>
</html>